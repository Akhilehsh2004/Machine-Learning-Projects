{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "374c18eb-4570-4c21-bd9b-2407dbe525fd",
   "metadata": {},
   "source": [
    "## Akhilesh Pant (AU FTCA: MCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6886bce5-a3a0-42c2-8166-374d20c48b51",
   "metadata": {},
   "source": [
    "## Self-Driving Car AI Object Detection Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b41ab8c9-a3f1-4f35-83de-613bdfbaf1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 2 persons, 297.3ms\n",
      "Speed: 6.3ms preprocess, 297.3ms inference, 5.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 234.8ms\n",
      "Speed: 3.4ms preprocess, 234.8ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 224.6ms\n",
      "Speed: 3.1ms preprocess, 224.6ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 236.6ms\n",
      "Speed: 3.6ms preprocess, 236.6ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 218.2ms\n",
      "Speed: 3.9ms preprocess, 218.2ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 232.2ms\n",
      "Speed: 5.3ms preprocess, 232.2ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 254.3ms\n",
      "Speed: 4.6ms preprocess, 254.3ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 219.5ms\n",
      "Speed: 4.1ms preprocess, 219.5ms inference, 3.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 232.1ms\n",
      "Speed: 4.0ms preprocess, 232.1ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 211.1ms\n",
      "Speed: 4.7ms preprocess, 211.1ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 211.6ms\n",
      "Speed: 3.8ms preprocess, 211.6ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 193.1ms\n",
      "Speed: 2.3ms preprocess, 193.1ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 209.4ms\n",
      "Speed: 4.5ms preprocess, 209.4ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 214.9ms\n",
      "Speed: 3.9ms preprocess, 214.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 225.7ms\n",
      "Speed: 4.4ms preprocess, 225.7ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 223.4ms\n",
      "Speed: 2.8ms preprocess, 223.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 208.4ms\n",
      "Speed: 2.6ms preprocess, 208.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 224.3ms\n",
      "Speed: 2.7ms preprocess, 224.3ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 219.2ms\n",
      "Speed: 6.9ms preprocess, 219.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 252.8ms\n",
      "Speed: 3.6ms preprocess, 252.8ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 270.8ms\n",
      "Speed: 3.3ms preprocess, 270.8ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 267.9ms\n",
      "Speed: 4.0ms preprocess, 267.9ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 247.4ms\n",
      "Speed: 4.3ms preprocess, 247.4ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 228.4ms\n",
      "Speed: 3.3ms preprocess, 228.4ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 240.0ms\n",
      "Speed: 3.6ms preprocess, 240.0ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 237.7ms\n",
      "Speed: 2.9ms preprocess, 237.7ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 250.4ms\n",
      "Speed: 3.0ms preprocess, 250.4ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 240.6ms\n",
      "Speed: 3.5ms preprocess, 240.6ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 242.8ms\n",
      "Speed: 25.5ms preprocess, 242.8ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 236.5ms\n",
      "Speed: 3.6ms preprocess, 236.5ms inference, 4.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 213.3ms\n",
      "Speed: 3.3ms preprocess, 213.3ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 205.7ms\n",
      "Speed: 3.2ms preprocess, 205.7ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 215.3ms\n",
      "Speed: 3.7ms preprocess, 215.3ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 219.5ms\n",
      "Speed: 3.6ms preprocess, 219.5ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 231.7ms\n",
      "Speed: 7.9ms preprocess, 231.7ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 205.1ms\n",
      "Speed: 3.3ms preprocess, 205.1ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 213.7ms\n",
      "Speed: 2.7ms preprocess, 213.7ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 213.1ms\n",
      "Speed: 2.4ms preprocess, 213.1ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 207.1ms\n",
      "Speed: 2.6ms preprocess, 207.1ms inference, 3.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 scissors, 201.1ms\n",
      "Speed: 2.7ms preprocess, 201.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 219.1ms\n",
      "Speed: 3.1ms preprocess, 219.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import threading\n",
    "import pyttsx3\n",
    "from ultralytics import YOLO\n",
    "from playsound import playsound\n",
    "from collections import deque\n",
    "\n",
    "# Load YOLOv8 model for object detection\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "# Initialize text-to-speech (TTS)\n",
    "tts_engine = pyttsx3.init()\n",
    "tts_engine.setProperty('rate', 160)\n",
    "\n",
    "# Assign random colors to each class\n",
    "np.random.seed(42)\n",
    "colors = {cls_id: tuple(np.random.randint(0, 255, 3).tolist()) for cls_id in range(80)}\n",
    "\n",
    "# Sound file path\n",
    "alert_sound = \"alert.wav\"\n",
    "\n",
    "# Object tracking memory (last N detections)\n",
    "object_history = deque(maxlen=10)\n",
    "\n",
    "def speak(text):\n",
    "    \"\"\"Speak text asynchronously using pyttsx3\"\"\"\n",
    "    def speak_thread():\n",
    "        tts_engine.say(text)\n",
    "        tts_engine.runAndWait()\n",
    "\n",
    "    threading.Thread(target=speak_thread, daemon=True).start()\n",
    "\n",
    "def play_alert_sound():\n",
    "    \"\"\"Play alert sound using playsound in a separate thread\"\"\"\n",
    "    def sound_thread():\n",
    "        try:\n",
    "            playsound(alert_sound)\n",
    "        except Exception as e:\n",
    "            print(f\"Error playing sound: {e}\")\n",
    "\n",
    "    threading.Thread(target=sound_thread, daemon=True).start()\n",
    "\n",
    "def detect_objects(frame):\n",
    "    \"\"\"Detect objects using YOLOv8\"\"\"\n",
    "    results = model(frame)\n",
    "    detected_objects = set()\n",
    "\n",
    "    for result in results:\n",
    "        if hasattr(result, \"boxes\") and result.boxes is not None:\n",
    "            for box in result.boxes:\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
    "                conf = box.conf[0].item()\n",
    "                cls = int(box.cls[0].item())\n",
    "                class_name = model.names.get(cls, 'Unknown')\n",
    "\n",
    "                detected_objects.add(class_name)\n",
    "                color = colors.get(cls, (0, 255, 0))\n",
    "\n",
    "                # Draw bounding box and label\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "                label = f\"{class_name} {conf:.2f}\"\n",
    "                cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "                # Traffic Sign Detection\n",
    "                if class_name in [\"stop sign\", \"traffic light\", \"speed limit\"]:\n",
    "                    speak(f\"Detected {class_name}\")\n",
    "\n",
    "    # Avoid duplicate alerts\n",
    "    if detected_objects and detected_objects not in object_history:\n",
    "        object_history.append(detected_objects)\n",
    "        speak(f\"Detected: {', '.join(detected_objects)}\")\n",
    "\n",
    "    return frame\n",
    "\n",
    "def detect_lanes(frame):\n",
    "    \"\"\"Lane detection using Canny Edge and Hough Transform\"\"\"\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    edges = cv2.Canny(blur, 50, 150)\n",
    "\n",
    "    height, width = frame.shape[:2]\n",
    "    mask = np.zeros_like(edges)\n",
    "    region_of_interest = np.array([[(50, height), (width // 2 - 50, height // 2), (width // 2 + 50, height // 2), (width - 50, height)]], np.int32)\n",
    "    cv2.fillPoly(mask, region_of_interest, 255)\n",
    "    \n",
    "    masked_edges = cv2.bitwise_and(edges, mask)\n",
    "\n",
    "    lines = cv2.HoughLinesP(masked_edges, 2, np.pi / 180, 100, np.array([]), minLineLength=50, maxLineGap=150)\n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            x1, y1, x2, y2 = line[0]\n",
    "            cv2.line(frame, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
    "\n",
    "    return frame\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not access webcam.\")\n",
    "else:\n",
    "    prev_time = time.time()\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Measure FPS\n",
    "        curr_time = time.time()\n",
    "        fps = 1 / (curr_time - prev_time)\n",
    "        prev_time = curr_time\n",
    "\n",
    "        # Detect objects and lanes\n",
    "        frame = detect_objects(frame)\n",
    "        frame = detect_lanes(frame)\n",
    "\n",
    "        # Display FPS and instructions\n",
    "        cv2.putText(frame, f\"FPS: {fps:.2f}\", (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)\n",
    "        cv2.putText(frame, \"Press 'q' to exit\", (20, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "\n",
    "        # Show window\n",
    "        cv2.imshow(\"Self-Driving AI\", frame)\n",
    "\n",
    "        # Quit on 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7024661-14a0-4205-84c9-a72e63033d32",
   "metadata": {},
   "source": [
    "## Explanation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fc6fae-fc4d-479b-82cb-1af9d7300070",
   "metadata": {},
   "source": [
    "This code is implementing **a real-time AI-powered self-driving assistant** using **YOLOv8 for object detection** and **Canny Edge Detection & Hough Transform for lane detection**. Additionally, it includes **text-to-speech (TTS) alerts** and plays warning sounds when specific objects are detected.  \n",
    "\n",
    "Let's go through it **line by line**:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Import Required Libraries**\n",
    "```python\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import threading\n",
    "import pyttsx3\n",
    "from ultralytics import YOLO\n",
    "from playsound import playsound\n",
    "from collections import deque\n",
    "```\n",
    "- **cv2** → OpenCV for computer vision tasks (image processing, webcam handling).  \n",
    "- **numpy** → Used for numerical operations (array manipulation).  \n",
    "- **torch** → PyTorch for deep learning model support.  \n",
    "- **time** → Used for FPS calculation and performance measurement.  \n",
    "- **threading** → Runs text-to-speech (TTS) and sound alert in separate threads to avoid lag.  \n",
    "- **pyttsx3** → A TTS library for converting text to speech.  \n",
    "- **YOLO** → Uses Ultralytics YOLOv8 for real-time object detection.  \n",
    "- **playsound** → Plays alert sounds when detecting important objects.  \n",
    "- **deque** → Keeps a history of detected objects (prevents repetitive alerts).  \n",
    "\n",
    "---\n",
    "\n",
    "### **2. Load YOLOv8 Model**\n",
    "```python\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "```\n",
    "- Loads the **YOLOv8 nano model (`yolov8n.pt`)**, which is optimized for fast real-time performance.  \n",
    "\n",
    "---\n",
    "\n",
    "### **3. Initialize Text-to-Speech (TTS)**\n",
    "```python\n",
    "tts_engine = pyttsx3.init()\n",
    "tts_engine.setProperty('rate', 160)\n",
    "```\n",
    "- Initializes the **TTS engine** and sets the speaking rate (160 words per minute).  \n",
    "\n",
    "---\n",
    "\n",
    "### **4. Assign Random Colors for Object Classes**\n",
    "```python\n",
    "np.random.seed(42)\n",
    "colors = {cls_id: tuple(np.random.randint(0, 255, 3).tolist()) for cls_id in range(80)}\n",
    "```\n",
    "- Assigns **random colors** to each of the 80 YOLO object classes for visualization.  \n",
    "- Each detected object gets a unique **bounding box color**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **5. Define Sound Alert File**\n",
    "```python\n",
    "alert_sound = \"alert.wav\"\n",
    "```\n",
    "- Specifies the sound file **\"alert.wav\"**, which will play when a critical object (like a traffic sign) is detected.  \n",
    "\n",
    "---\n",
    "\n",
    "### **6. Create Object Tracking Memory**\n",
    "```python\n",
    "object_history = deque(maxlen=10)\n",
    "```\n",
    "- Keeps track of the **last 10 detected objects** to avoid repeated announcements.  \n",
    "\n",
    "---\n",
    "\n",
    "### **7. Function: Speak Alert Messages**\n",
    "```python\n",
    "def speak(text):\n",
    "    def speak_thread():\n",
    "        tts_engine.say(text)\n",
    "        tts_engine.runAndWait()\n",
    "    threading.Thread(target=speak_thread, daemon=True).start()\n",
    "```\n",
    "- Uses **multithreading** to speak detection alerts **without blocking the main program**.  \n",
    "- Example: If a **stop sign** is detected, the system will announce:  \n",
    "  👉 *\"Detected Stop Sign\"*  \n",
    "\n",
    "---\n",
    "\n",
    "### **8. Function: Play Alert Sound**\n",
    "```python\n",
    "def play_alert_sound():\n",
    "    def sound_thread():\n",
    "        try:\n",
    "            playsound(alert_sound)\n",
    "        except Exception as e:\n",
    "            print(f\"Error playing sound: {e}\")\n",
    "    threading.Thread(target=sound_thread, daemon=True).start()\n",
    "```\n",
    "- Plays a **warning sound (`alert.wav`)** in a separate thread to avoid lag.  \n",
    "\n",
    "---\n",
    "\n",
    "### **9. Function: Detect Objects Using YOLO**\n",
    "```python\n",
    "def detect_objects(frame):\n",
    "    results = model(frame)\n",
    "    detected_objects = set()\n",
    "```\n",
    "- Runs the **YOLOv8 model** on the **current video frame** to detect objects.  \n",
    "- Stores detected objects in a **set** to avoid duplicate alerts.  \n",
    "\n",
    "---\n",
    "\n",
    "### **10. Process YOLO Detections**\n",
    "```python\n",
    "for result in results:\n",
    "    if hasattr(result, \"boxes\") and result.boxes is not None:\n",
    "        for box in result.boxes:\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
    "            conf = box.conf[0].item()\n",
    "            cls = int(box.cls[0].item())\n",
    "            class_name = model.names.get(cls, 'Unknown')\n",
    "\n",
    "            detected_objects.add(class_name)\n",
    "            color = colors.get(cls, (0, 255, 0))\n",
    "```\n",
    "- Loops through **each detected object** and extracts:  \n",
    "  - **Bounding box coordinates** `(x1, y1, x2, y2)`  \n",
    "  - **Confidence score** `conf`  \n",
    "  - **Class ID** `cls`  \n",
    "  - **Object name** `class_name`  \n",
    "\n",
    "- The object is added to `detected_objects` and assigned a **random color**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **11. Draw Bounding Boxes and Labels**\n",
    "```python\n",
    "cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "label = f\"{class_name} {conf:.2f}\"\n",
    "cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "```\n",
    "- Draws a **bounding box** and labels each detected object on the frame.  \n",
    "\n",
    "---\n",
    "\n",
    "### **12. Detect Traffic Signs & Speak Alerts**\n",
    "```python\n",
    "if class_name in [\"stop sign\", \"traffic light\", \"speed limit\"]:\n",
    "    speak(f\"Detected {class_name}\")\n",
    "```\n",
    "- If a **traffic-related sign** is detected, the system **announces it via TTS**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **13. Prevent Duplicate Alerts**\n",
    "```python\n",
    "if detected_objects and detected_objects not in object_history:\n",
    "    object_history.append(detected_objects)\n",
    "    speak(f\"Detected: {', '.join(detected_objects)}\")\n",
    "```\n",
    "- Checks if the **same object** was detected recently (avoids repeated alerts).  \n",
    "\n",
    "---\n",
    "\n",
    "### **14. Function: Lane Detection**\n",
    "```python\n",
    "def detect_lanes(frame):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    edges = cv2.Canny(blur, 50, 150)\n",
    "```\n",
    "- Converts the frame to **grayscale** → **Applies Gaussian Blur** → **Detects edges using Canny Edge Detection**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **15. Define Region of Interest & Detect Lines**\n",
    "```python\n",
    "height, width = frame.shape[:2]\n",
    "mask = np.zeros_like(edges)\n",
    "region_of_interest = np.array([[(50, height), (width // 2 - 50, height // 2), (width // 2 + 50, height // 2), (width - 50, height)]], np.int32)\n",
    "cv2.fillPoly(mask, region_of_interest, 255)\n",
    "\n",
    "masked_edges = cv2.bitwise_and(edges, mask)\n",
    "```\n",
    "- Defines the **region of interest (ROI)** → Focuses only on the road area.  \n",
    "\n",
    "```python\n",
    "lines = cv2.HoughLinesP(masked_edges, 2, np.pi / 180, 100, np.array([]), minLineLength=50, maxLineGap=150)\n",
    "if lines is not None:\n",
    "    for line in lines:\n",
    "        x1, y1, x2, y2 = line[0]\n",
    "        cv2.line(frame, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
    "```\n",
    "- Uses **Hough Transform** to detect **lane lines** and draw them in green.  \n",
    "\n",
    "---\n",
    "\n",
    "### **16. Capture Video From Webcam**\n",
    "```python\n",
    "cap = cv2.VideoCapture(0)\n",
    "```\n",
    "- Opens the **webcam** for real-time video processing.  \n",
    "\n",
    "---\n",
    "\n",
    "### **17. Main Loop: Process Each Frame**\n",
    "```python\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "```\n",
    "- Reads **each frame** from the webcam **until the user quits**.  \n",
    "\n",
    "```python\n",
    "frame = detect_objects(frame)\n",
    "frame = detect_lanes(frame)\n",
    "```\n",
    "- Runs **object detection & lane detection** on each frame.  \n",
    "\n",
    "```python\n",
    "cv2.imshow(\"Self-Driving AI\", frame)\n",
    "if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "    break\n",
    "```\n",
    "- Displays the processed video and **exits when 'q' is pressed**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion**\n",
    "✅ **YOLOv8 detects objects in real-time**  \n",
    "✅ **Lane detection highlights road lanes**  \n",
    "✅ **TTS announces traffic signs & plays alerts**  \n",
    "✅ **Runs smoothly without freezing (multithreading)**  \n",
    "\n",
    "This is an **AI-powered self-driving assistant** 🚗💡"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
